# -*- coding: utf-8 -*-
"""Dorne_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11swP6U-pDi2Y8n0642hcpTSlHbobu0YZ

## Not rellevant anymore
"""


'''
from pydub import AudioSegment
from pydub.utils import make_chunks
'''
import tkinter as tk
from tkinter import filedialog, messagebox
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
#from sklearn.preprocessing import StandardScaler
import librosa
from pathlib import Path
from pydub import AudioSegment
import os
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import wave
import joblib
# Evaluation metrics
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix
)
from sklearn import svm
import time
from tkinter import PhotoImage
from PIL import Image, ImageTk


# Global variables to store file paths
drone_features_path = None
no_drone_features_path = None
audio_file_path = None


def plot_audio_signal_and_prediction(audio_file_path, prediction):
  # Load and process the wave file
  spf = wave.open(audio_file_path, "r")

  # Extract Raw Audio from Wav File
  signal = spf.readframes(-1)
  signal = np.frombuffer(signal, np.int16)  # Updated method
  fs = spf.getframerate()

  # Normalize the signal to range [-1, 1]
  signal = signal / np.max(np.abs(signal))

  # Time for the audio signal
  Time = np.linspace(0, len(signal) / fs, num=len(signal))

  # Prediction list (scaled to match the signal length)
  # prediction = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]
  prediction_time = np.linspace(0, len(signal) / fs, num=len(prediction))  # Align with audio time

  # Upsample the prediction array to match the length of the signal
  upsampled_prediction = np.interp(Time, prediction_time, prediction)

  # # Plot both graphs
  # plt.figure(figsize=(12, 6))
  # plt.plot(Time, signal, label="Normalized Audio Signal", color="blue")
  # plt.plot(Time, upsampled_prediction, label="Drone Detection", color="red", linewidth=2)
  # Plot both graphs
  plt.figure(figsize=(12, 6))
  plt.plot(Time, signal, label="Audio Signal", color="blue")
  plt.step(prediction_time, prediction, where='post', label="Drone Detection", color="red")

  # Add labels, legend, and grid
  plt.title("Normalized Audio Signal and Drone Detection")
  plt.xlabel("Time (seconds)")
  plt.ylabel("Normalized Amplitude / Detection")
  plt.legend()
  plt.grid(True, which='both', linestyle='--', linewidth=0.5)
  plt.ylim(-1.5, 1.5)  # Set consistent y-axis limits
  plt.show()

def chunk_audio_file(input_file, chunk_length_ms=10000, output_dir="chunks"):
    """
    Divide audio file into chunks and save them to a directory
    """
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    audio = AudioSegment.from_wav(input_file)
    base_filename = os.path.splitext(os.path.basename(input_file))[0]

    chunks = [audio[i:i + chunk_length_ms]
             for i in range(0, len(audio), chunk_length_ms)]

    chunk_files = []
    for i, chunk in enumerate(chunks):
        chunk_name = f"{base_filename}-chunk_{i:04d}.wav"
        chunk_path = os.path.join(output_dir, chunk_name)

        chunk.export(chunk_path, format="wav")
        chunk_files.append(chunk_path)

    return chunk_files

def extract_features_from_wav(wav_path):
    """
    Extract audio features from a single WAV file
    """
    y, sr = librosa.load(wav_path, mono=True, duration=30)

    # return {
    #     'filename': os.path.basename(wav_path),
    #     'chroma_stft_mean': np.mean(librosa.feature.chroma_stft(y=y, sr=sr)),
    #     'spectral_centroid_mean': np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)),
    #     'spectral_bandwidth_mean': np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr)),
    #     'rolloff_mean': np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr)),
    #     'zero_crossing_rate_mean': np.mean(librosa.feature.zero_crossing_rate(y=y)),
    # }
    # Extract MFCC features
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    n_mfcc = 13
    # Add MFCC features to the dictionary
    mfcc_features = {}
    for i in range(n_mfcc):
        mfcc_features[f'mfcc{i+1}_mean'] = np.mean(mfcc[i])
        mfcc_features[f'mfcc{i+1}_std'] = np.std(mfcc[i])

    rest_of_the_features = {
        'filename': os.path.basename(wav_path),
        'chroma_stft_mean': np.mean(librosa.feature.chroma_stft(y=y, sr=sr)),
        'spectral_centroid_mean': np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)),
        'spectral_bandwidth_mean': np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr)),
        'rolloff_mean': np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr)),
        'zero_crossing_rate_mean': np.mean(librosa.feature.zero_crossing_rate(y=y)),
    }

    # Combine all features
    all_features = {**rest_of_the_features, **mfcc_features}

    return all_features

def create_feature_df(wav_path, chunk_length_ms, chunks_dir):
    all_features = []
    # Chunk the audio file
    chunk_files = chunk_audio_file(
        wav_path,
        chunk_length_ms=chunk_length_ms,
        output_dir=chunks_dir
    )

    # Extract features for each chunk
    for chunk_file in chunk_files:
        features = extract_features_from_wav(chunk_file)
        #features['original_wav'] = file
        all_features.append(features)
        #print(f"Processed chunk: {chunk_file}")

        # Convert features to DataFrame
    features_df = pd.DataFrame(all_features)

    return features_df

# def process_audio_file(audio_path, sr=22050, duration=None):
#     """
#     Process a single audio file and extract the same features used in training
#     """
#     # Load audio file
#     y, sr = librosa.load(audio_path)

#     # Extract features (make sure these are the same features you used for training)
#     features = {
#         'rms': np.mean(librosa.feature.rms(y=y)),
#         'zero_crossing_rate': np.mean(librosa.feature.zero_crossing_rate(y=y)),
#         'spectral_centroid': np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)),
#         'spectral_rolloff': np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr)),
#         'spectral_bandwidth': np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))
#         # Add any other features you used during training
#     }

#     return pd.DataFrame([features])

def predict_drone(audio_path, model, chunk_length_ms, chunks_dir):
    """
    Predict whether an audio file contains drone noise
    """
    # Process the audio file
    #features_df = process_audio_file(audio_path)
    features_df = create_feature_df(audio_path, chunk_length_ms, chunks_dir)
    #print(features_df.head())
    features_df = features_df.drop(columns=["filename"])
    features_df.to_csv("recorded_features.csv")

    # Make prediction
    prediction = model.predict(features_df)
    probability = model.predict_proba(features_df)
    #print(probability)
    # print(type(prediction))
    print(prediction)
    majority_label = np.bincount(prediction).argmax()
    # print(majority_label)
    #majority_label = max(l)

    # Calculate mean of each column
    label_0_prob_mean = np.mean(probability[:, 0])   # First column
    label_1_prob_mean = np.mean(probability[:, 1])  # Second column

    if majority_label == 1:
      conf = label_1_prob_mean
    else:
      conf = label_0_prob_mean

    plot_audio_signal_and_prediction(audio_path, prediction)
    return {
        #'is_drone': bool(prediction[0]),
        'is_drone': bool(majority_label),
        #'confidence': probability[0][1] if prediction[0] else probability[0][0]
        'confidence': conf
    }




# Example usage:
def main():
    # Load your trained model and scaler (assuming you saved them)


    # Load the saved model and scaler
    model = joblib.load('drone_detector_model.joblib')

    # Test on new audio file
    audio_path = audio_file_path
    chunk_length_ms = 1000
    chunks_dir = "chunks"
    result = predict_drone(audio_path, model, chunk_length_ms, chunks_dir)

    # if result['is_drone']:
    #     print(f"Drone detected with {result['confidence']*100:.2f}% confidence")
    # else:
    #     print(f"No drone detected with {result['confidence']*100:.2f}% confidence")

# # Save your model and scaler (do this after training)
# import joblib
# joblib.dump(clf_svm, 'drone_detector_model.joblib')


def upload_file(file_type, text_box, file_extensions, variable_name):
    global drone_features_path, no_drone_features_path, audio_file_path

    # Open a file dialog to select a file
    file_path = filedialog.askopenfilename(
        title=f"Select a {file_type}",
        filetypes=[(f"{file_type} Files", file_extensions), ("All Files", "*.*")]
    )

    if file_path:
        # Update the corresponding text box with the selected file path
        text_box.delete(1.0, tk.END)  # Clear existing content
        text_box.insert(tk.END, file_path)

        # Assign the path to the specific variable
        if variable_name == "drone_features":
            drone_features_path = file_path
        elif variable_name == "no_drone_features":
            no_drone_features_path = file_path
        elif variable_name == "audio_file":
            audio_file_path = file_path

        print(f"{file_type} path set to: {file_path}")  # Debugging output

        # Check if all files are uploaded to enable the "Run" button
        if drone_features_path and no_drone_features_path and audio_file_path:
            run_button.config(state=tk.NORMAL)




def run_code():
    # Ensure all files are selected
    if not (drone_features_path and no_drone_features_path and audio_file_path):
        messagebox.showerror("Error", "Please upload all required files before running.")
        return

    # Perform actions with the uploaded files
    print("\nRunning the program with the following files:")
    print(f"Drone Features Path: {drone_features_path}")
    print(f"No Drone Features Path: {no_drone_features_path}")
    print(f"Audio File Path: {audio_file_path}")

    """CSV UPLOAD"""

    # Load datasets
    drone_df = pd.read_csv(drone_features_path)
    no_drone_df = pd.read_csv(no_drone_features_path)

    # Add label column (if not already done)
    drone_df['label'] = 1  # 1 for drone
    no_drone_df['label'] = 0  # 0 for no drone

    drone_df.head()

    # drone_df.drop(columns=["filename", "original_wav"])
    # no_drone_df.drop(columns=["filename", "original_wav"])

    """DATA ARAINGING

    """

    drone_df = drone_df.drop(columns=["filename", "original_wav"])
    no_drone_df = no_drone_df.drop(columns=["filename", "original_wav"])
    # Combine datasets
    combined_df = pd.concat([drone_df, no_drone_df], ignore_index=True)

    # Shuffle the data
    combined_df = combined_df.sample(frac=1).reset_index(drop=True)

    # Separate features and labels
    X = combined_df.drop('label', axis=1)
    y = combined_df['label']

    column_names = X.columns
    print(column_names)

    print(y.head())
    # Split into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    print(combined_df.shape[0])
    combined_df.head()
    # # Scale features
    # scaler = StandardScaler()
    # X_train_scaled = scaler.fit_transform(X_train)
    # X_test_scaled = scaler.transform(X_test)

    X.head()

    y.head()

    print(y.value_counts())

    # # Scale features
    # scaler = StandardScaler()
    # X_train_scaled = scaler.fit_transform(X_train)
    # X_test_scaled = scaler.transform(X_test)

    """Rendom Forest"""

    # Train a Random Forest Classifier
    clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)
    # clf.fit(X_train_scaled, y_train)
    clf_rf.fit(X_train, y_train)

    # Evaluate the model
    # y_pred = clf.predict(X_test_scaled)
    y_pred = clf_rf.predict(X_test)
    print(classification_report(y_test, y_pred))
    print(confusion_matrix(y_test, y_pred))

    # Predict on test data
    # y_pred = clf.predict(X_test_scaled)
    # y_pred = clf.predict(X_test)

    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred))
    print("Recall:", recall_score(y_test, y_pred))
    print("F1 Score:", f1_score(y_test, y_pred))
    print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

    # # Optional: Test on new data
    # new_data_scaled = scaler.transform(X_test)
    # predictions = clf.predict(new_data_scaled)

    """SVM MODEL"""

    clf_svm = svm.SVC(probability=True)
    # clf.fit(X_train_scaled, y_train)
    clf_svm.fit(X_train, y_train)

    # Evaluate the model
    # y_pred = clf.predict(X_test_scaled)
    y_pred = clf_svm.predict(X_test)
    print(classification_report(y_test, y_pred))
    print(confusion_matrix(y_test, y_pred))

    # Predict on test data
    # y_pred = clf.predict(X_test_scaled)
    # y_pred = clf.predict(X_test)

    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred))
    print("Recall:", recall_score(y_test, y_pred))
    print("F1 Score:", f1_score(y_test, y_pred))
    print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

    # # Optional: Test on new data
    # new_data_scaled = scaler.transform(X_test)
    # predictions = clf.predict(new_data_scaled)

    """## Using the model"""

    joblib.dump(clf_svm, 'drone_detector_model.joblib')

    """RECORDING TETS"""

    main()

    # Example: Your processing logic here
    messagebox.showinfo("Run Complete", "The program has executed successfully!")


# Initialize the main application window
app = tk.Tk()
app.title("Drone Detection")
app.geometry("880x250")



# Get the directory where the script (or .exe) is located
current_directory = os.path.dirname(os.path.abspath(__file__))


# Construct the path to the image file in the same directory
icon_path = os.path.join(current_directory, "drone_icon.png")
# Set the icon (ensure the path to your image is correct)
icon = PhotoImage(file=icon_path)  # Example for PNG files
app.iconphoto(True, icon)  # Set the icon

# Load the background image (sky.jpg) from the same directory
bg_image_path = os.path.join(current_directory, "sky.jpg")
bg_image = Image.open(bg_image_path)

# Resize the image to fit the window size
bg_image = bg_image.resize((app.winfo_screenwidth(), app.winfo_screenheight()), Image.ANTIALIAS)

# Convert the image to a Tkinter-compatible format
bg_image_tk = ImageTk.PhotoImage(bg_image)

# Create a Label widget to display the background image
background_label = tk.Label(app, image=bg_image_tk)
background_label.place(relwidth=1, relheight=1)  # Fill the entire window


# Create labels, buttons, and text boxes for each file type
# 1. Drone Features CSV
drone_label = tk.Label(app, text="Upload Drone Features CSV:", font=("Arial", 12))
drone_label.grid(row=0, column=0, padx=10, pady=10, sticky="w")

drone_button = tk.Button(
    app,
    text="Upload Drone CSV", bg="white", fg="black", font=("Helvetica", 14),
    command=lambda: upload_file("Drone Features CSV", drone_text, "*.csv", "drone_features")
)
drone_button.grid(row=0, column=1, padx=10, pady=10)


drone_text = tk.Text(app, height=1, width=50, font=("Arial", 10))
drone_text.grid(row=0, column=2, padx=10, pady=10)

# 2. No Drone Features CSV
no_drone_label = tk.Label(app, text="Upload No Drone Features CSV:", font=("Arial", 12))
no_drone_label.grid(row=1, column=0, padx=10, pady=10, sticky="w")

no_drone_button = tk.Button(
    app,
    text="Upload No Drone CSV", bg="white", fg="black", font=("Helvetica", 14),
    command=lambda: upload_file("No Drone Features CSV", no_drone_text, "*.csv", "no_drone_features"),

)
no_drone_button.grid(row=1, column=1, padx=10, pady=10)

no_drone_text = tk.Text(app, height=1, width=50, font=("Arial", 10))
no_drone_text.grid(row=1, column=2, padx=10, pady=10)

# 3. Audio File
audio_label = tk.Label(app, text="Upload an Audio File:", font=("Arial", 12))
audio_label.grid(row=2, column=0, padx=10, pady=10, sticky="w")

audio_button = tk.Button(
    app,
    text="Upload Audio File", bg="white", fg="black", font=("Helvetica", 14),
    command=lambda: upload_file("Audio File", audio_text, "*.wav", "audio_file"),
)
audio_button.grid(row=2, column=1, padx=10, pady=10)

audio_text = tk.Text(app, height=1, width=50, font=("Arial", 10))
audio_text.grid(row=2, column=2, padx=10, pady=10)

# Run Button
run_button = tk.Button(
    app,
    text="Run", bg="white", fg="black", font=("Helvetica", 14),
    command=run_code,
    state=tk.DISABLED
)
run_button.grid(row=3, column=1, pady=20)

# Run the main loop
app.mainloop()




